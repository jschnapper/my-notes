# 3/13
## Probability 

### Background 
**Sample**: data we have

**Population**: the group we want to study that we don't have entire access to

**Generalization**: going from data to world (sample to population)

**Data Generation Process**: how the sample is collected from the population (population to sample)

### Probability
**random variable**: variable whose value is determined by a chance event   

**chance event**: outcome cannot be inferred. It is a random process.

**Probability Mass Function (PMF)**: The discrete joint probability distribution
* the probability a variable will take on a particular value
* 

**Expected Value**: the mean (something about the distribution). A quantity, not a random variable. Is represented as:

$$ E[X] = \sum_{x \in X}xP(x) $$

#### Sampling
Independent events = take product
**With Replacement**
**Without Replacement**

***sample mean** is a random variable*


Use **Linearity of Expectation**  when determining the value of a random variable.
(lowercase letters = constants, not random)
$$ E[aX + Y + b] = aE[X] + E[Y] + b $$

* linearity of a constant is a constant itself $E[25] = 25$
* independence not required (applies to both sampling with and withour replacement)

**Conditional Definition** 
$$P(x,y) = P(x|y)P(y) = P(y|x)P(x)$$
* everything on the left side of a conditional sums to 1 

* If X and Y are ***independent*** then it can be said that $E[XY] = E[X]E[Y]$
  * but it is not guaranteed that if $E[X]E[Y] = E[XY] that they are independent 
  * **As long as it is stated that they are independent, you can use this**

#### Variance
**The spread of the variable about the mean where x is value in the probability distribution of X**

$$ Var[X] = E[(X - E[X])^2] = \sum_{x \in X} (x - E[X])^{2}P(x) $$

Identity
$$ Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2 $$

Properties
$$ Var[aX + b] = a^2Var[X] + 0 $$

***If X and Y are independent***
$$ Var[X+Y] = Var[X] + Var[Y] $$ 

Variance is actually in squared units. Can get **Standard Deviation**
$$ SD[X] = \sqrt{Var[X]} $$

Another identity
$$ SD[aX + b] = |a| SD[x] $$

#### Covariance
Describes how 2 variables vary jointly
$$ Cov[X,Y] = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$$

Basic Properties
$$ Cov[aX + u, bY = v] = abCov[X,Y] $$

If X and Y are ***independent*** then:
$$ E[XY] = E[X]E[Y] $$ 
which would show that the $Cov[X,Y] = 0$ when variables are independent

#### Correlation
Units of covariance can be difficult to reason about 
**Correlation** is the normalized covariance 

$$ \rho X,Y = Corr[X,Y] = \frac{Cov[X,Y]}{\sqrt{Var[X]}\sqrt{[Var[Y]}}
= \frac{Cov[X,Y]}{SD[X]SD[Y]} $$

Also depicted as Rho X,Y

* Value of correlaton is always between -1 and 1
* If negative correlation, will have graph a line with a negative slope. If positive correlation, will be along line of positive slope

#### Standard Error
the standard deviation of the sample mean (mean on the sample)

$$ SE(\overline{X}) = \sqrt{Var[\overline{X}]} = \frac{\sigma}{\sqrt{n}} $$


#### Bernoulli
Bernoulli proability is **1** with probability **p** and 0 with probability **1-p**


#### in practice
Expected value of **sample mean** is the **population mean**. A sample mean is an **unbiased estimator** of the population mean. 
$$ Bias = E[\overline{X}] - \mu = 0 $$

The **variance** of the **sample mean** decreases at a **rate of one over the sample size**
$$ Var[\overline{X} = \frac{p^{* }(1-p^{* })}{n}] $$

#### Probability mass function (Sampling Distribution) of  $\overline{X}$

Taking multiple sample means from a distribution. Each one will be different. Each sample is random.

#### Central Limit Theorem
The sample average behaves approximately like a random draw from the normal distribution
* assumes independent and identically distributed observations

Bigger and bigger samples converge to mean: **Law of Large Numbers**. As sample size grows, sample mean approaches population mean. 

#### Bootstrap the Distribution of an Estimator
simulation method to estimate the sample distribution
**steps**:
* obtain sample; pretend this is your population
* sample with replacement from this population -- these are the **bootstrap samples** 
* take these samples and compute stats on them to get sample mean
* Get empirical distribution that fairly closely reflects the distribution would have gotten had you sampled from actual population  
* Can define confidence interval with bootstrap 

#### connection to loss minimization
Average Loss:

$$ L_{avg}(\Theta) = \frac{1}{n} \sum_{i=1}^{n}l(Y_i,f_\theta(X_i)) $$

This is a **sample loss**. A random variable (depending on Xi and Yi)

**risk and expected loss**
**Risk** is a number, not a random variable, that is the expectiation of loss over a population. The risk associated with the choice of $\Theta$
$$ R(\Theta) = E[l(Y,f_\Theta(X))] $$
Given access to joint probability of X and Y, can rewrite risk as:
$$ R(\Theta) = \sum_{x \in X} \sum_{y \in Y} l(y, f_\Theta(x))P(x,y) $$

Becomes estimate of empirical risk. 

**The average loss is the approximation of the risk function**





  

